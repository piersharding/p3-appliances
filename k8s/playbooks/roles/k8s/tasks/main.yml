---

- name: Set the host grouping
  set_fact:  
    host_grouping: "{% if inventory_hostname != groups['k8s_master'][0] %}master{% else %}worker{% endif %}"

- name: set cluster API address.
  set_fact:
    # cluster_api_address: "{{ hostvars[groups['k8s_master'][0]].ansible_default_ipv4.address }}"
    cluster_api_address: "{{ hostvars[groups['k8s_master'][0]]['secondary_ip'] if ('secondary_ip' in hostvars[groups['k8s_master'][0]]) else hostvars[groups['k8s_master'][0]]['ansible_default_ipv4']['address'] }}"
  # when: "inventory_hostname in groups['k8s_master']"


- name: Add k8s apt key
  apt_key:
    url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
    state: present

- name: Add k8s apt source
  lineinfile:
    line: 'deb http://apt.kubernetes.io/ kubernetes-xenial main'
    path: /etc/apt/sources.list.d/k8s.list
    create: yes
    owner: root
    group: root
    mode: 0644

- name: Update apt cache - again
  apt:
    update_cache: yes

# - name: Install kubeadm
#   apt:
#     name: "{{ item }}"
#     force: yes
#     state: present
#   with_items:
#     - "kubernetes-cni={{ kubernetes_cni_version }}"
#     - "kubelet={{ kubelet_version }}"
#     - "kubectl={{ kubectl_version }}"
#     - "kubeadm={{ kubeadm_version }}"
#   notify:
#     - restart kubelet

- name: Install kubeadm
  apt:
    name: "{{ item }}"
    force: yes
    state: present
  with_items:
    # - "kubernetes-cni"
    - "kubelet"
    - "kubectl"
    - "kubeadm"
  notify:
    - restart kubelet

- name: Configure calico 
  template: src=calico-3.1.yaml.j2 dest="/home/ubuntu/calico-3.1.yaml"
  when: inventory_hostname in groups['k8s_master']

# - meta: flush_handlers


# - name: "Show nodes info"
#   debug: var=hostvars[inventory_hostname]
#   when: debug

- name: set cluster API address.
  set_fact:
    # cluster_api_address: "{{ hostvars[groups['k8s_master'][0]].ansible_default_ipv4.address }}"
    cluster_api_address: "{{ hostvars[groups['k8s_master'][0]]['secondary_ip'] if ('secondary_ip' in hostvars[groups['k8s_master'][0]]) else hostvars[groups['k8s_master'][0]]['ansible_default_ipv4']['address'] }}"
    cluster_address_list: "{{ hostvars[groups['k8s_master'][0]]['ansible_default_ipv4']['address'] }}"
  # when: "inventory_hostname in groups['k8s_master']"

- name:
  stat: path=/etc/kubernetes/admin.conf
  register: admin_conf
  delegate_to: "{{ groups['k8s_master'][0] }}"
  delegate_facts: True

- name: "Show cluster_api_address"
  debug: var=cluster_api_address
  when: debug

- name: Check if "Cluster is active" is enabled.
  shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf version
  changed_when: False
  register: kubectl_version
  ignore_errors: true
  delegate_to: "{{ groups['k8s_master'][0] }}"
  delegate_facts: True
  # when: "inventory_hostname in groups['k8s_master']"
  tags:
    - skip_ansible_lint # Suppressing the linter

- name: "Show kubectl_version"
  debug: var=kubectl_version
  when: debug

- name: Init Cluster on the first master.
  shell: /usr/bin/kubeadm init 
        --pod-network-cidr=192.168.0.0/16 
        --apiserver-advertise-address {{ cluster_api_address }}
        --apiserver-cert-extra-sans {{ cluster_address_list }}
  when: "kubectl_version.stdout.find('Server Version:') == -1
    and inventory_hostname in groups['k8s_master']"
  tags:
    - skip_ansible_lint # Suppressing the linter

- name: Wait for API Server to come up
  wait_for:
    host: "{{ cluster_api_address }}"
    port: 6443
    delay: 10

- name: retrieve kubectl config
  fetch: src="/etc/kubernetes/admin.conf" dest="/tmp/admin.conf" flat=yes
  when: "inventory_hostname in groups['k8s_master']"

- name: Ensure the /home/ubuntu/.kube directory exists
  file:
    path: /home/ubuntu/.kube
    state: directory
    mode: 0700
    owner: ubuntu
    group: ubuntu

- name: Setup kubectl config
  copy:
    src: /tmp/admin.conf
    dest: /home/ubuntu/.kube/config
    mode: 0644
    owner: ubuntu
    group: ubuntu

- name: Check if "Networking is active"
  shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get deployment calico-kube-controllers --namespace kube-system
  changed_when: False
  register: kubectl_calico
  ignore_errors: true
  delegate_to: "{{ groups['k8s_master'][0] }}"
  delegate_facts: True
  # when: "inventory_hostname in groups['k8s_master']"
  tags:
    - skip_ansible_lint # Suppressing the linter

- name: "Show kubectl_calico"
  debug: var=kubectl_calico
  when: debug

- name: Init Cluster networking on the first master.
  shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf 
         apply -f /home/ubuntu/calico-3.1.yaml
  when: "not kubectl_calico.rc == 0
    and inventory_hostname in groups['k8s_master']"
  tags:
    - skip_ansible_lint # Suppressing the linter

- name: Wait for Networking to come up
  shell: "kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods --field-selector 'status.phase!=Running' --namespace kube-system"
  register: calico_check
  until: calico_check.stdout_lines | reject('search','^No resources found') | list | count == 0
  retries: 10
  delay: 30
  # delegate_to: "{{ groups['k8s_master'][0] }}"
  when: "inventory_hostname in groups['k8s_master']"

- name: Get Join token
  shell: /usr/bin/kubeadm token list | grep -v TOKEN | head -1 | awk '{print $1}'
  changed_when: False
  register: kubectl_token
  ignore_errors: true
  delegate_to: "{{ groups['k8s_master'][0] }}"
  delegate_facts: True
  when: "not inventory_hostname in groups['k8s_master']"
  tags:
    - skip_ansible_lint # Suppressing the linter

- name: "Show kubectl_token"
  debug: var=kubectl_token
  when: debug

- name: Check already joined
  shell: "kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes | grep {{ inventory_hostname }}"
  changed_when: False
  register: kubectl_joined
  ignore_errors: true
  delegate_to: "{{ groups['k8s_master'][0] }}"
  delegate_facts: True
  # when: "inventory_hostname in groups['k8s_master']"
  tags:
    - skip_ansible_lint # Suppressing the linter

- name: "Show kubectl_joined"
  debug: var=kubectl_joined
  when: debug

- name: Init minions.
  shell: "/usr/bin/kubeadm join 
         --token {{ kubectl_token.stdout }} {{ cluster_api_address }}:6443
         --discovery-token-unsafe-skip-ca-verification"
  when: "not inventory_hostname in groups['k8s_master']
         and (kubectl_joined.stdout_lines | list | count == 0)"
  tags:
    - skip_ansible_lint # Suppressing the linter

- name: Ensure the /etc/kubernetes/manifests directory exists
  file:
    path: /etc/kubernetes/manifests
    state: directory
    mode: 0755
  when: "not inventory_hostname in groups['k8s_master']"

- name: Check joining
  shell: "kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes | grep {{ inventory_hostname }}"
  changed_when: False
  register: kubectl_joined
  delegate_to: "{{ groups['k8s_master'][0] }}"
  when: "not inventory_hostname in groups['k8s_master']"
  tags:
    - skip_ansible_lint # Suppressing the linter

- name: "Show kubectl_joined"
  debug: var=kubectl_joined
  when: debug

- name: Wait for nodes to join
  shell: "kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes  | grep {{ inventory_hostname }}"
  register: calico_check
  until: calico_check|success
  retries: 10
  delay: 30
  delegate_to: "{{ groups['k8s_master'][0] }}"
  when: "not inventory_hostname in groups['k8s_master']"

- name: Wait for calico to settle
  shell: "kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods --field-selector 'status.phase!=Running' --namespace kube-system"
  register: calico_check
  until: calico_check.stdout_lines | reject('search','^No resources found') | list | count == 0
  retries: 10
  delay: 30
  when: "inventory_hostname in groups['k8s_master']"

- name: /home/ubuntu/busybox.yml
  copy:
    content: |-
      apiVersion: v1
      kind: Pod
      metadata:
        name: busybox
        namespace: default
        annotations:
          scheduler.alpha.kubernetes.io/affinity: >
            {
              "nodeAffinity": {
                "requiredDuringSchedulingIgnoredDuringExecution": {
                  "nodeSelectorTerms": [
                    {
                      "matchExpressions": [
                        {
                          "key": "kubernetes.io/hostname",
                          "operator": "NotIn",
                          "values": ["k8smaster"]
                        }
                      ]
                    }
                  ]
                }
              }
            }
      spec:
        containers:
        - image: busybox
          command:
            - sleep
            - "3600"
          imagePullPolicy: IfNotPresent
          name: busybox
        restartPolicy: Always

    force: yes
    dest: /home/ubuntu/busybox.yml

- name: Check if busybox is up
  shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get pod busybox
  changed_when: False
  register: kubectl_busbox
  ignore_errors: true
  when: "inventory_hostname in groups['k8s_master']"
  tags:
    - skip_ansible_lint # Suppressing the linter

- name: Launch busybox
  shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf 
         apply -f /home/ubuntu/busybox.yml
  when: "inventory_hostname in groups['k8s_master']
         and not kubectl_busbox.rc == 0"
  tags:
    - skip_ansible_lint # Suppressing the linter

- name: Wait for busybox to come up
  shell: "kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods --field-selector 'status.phase!=Running' --all-namespaces"
  register: busybox_check
  until: busybox_check.stdout_lines | reject('search','^No resources found') | list | count == 0
  retries: 10
  delay: 30
  # delegate_to: "{{ groups['k8s_master'][0] }}"
  when: "inventory_hostname in groups['k8s_master']"

# state metrics required for:
# https://www.elastic.co/guide/en/beats/metricbeat/current/metricbeat-module-kubernetes.html
# https://github.com/kubernetes/kube-state-metrics/tree/master/kubernetes
- name: Configure kube-state-metrics DaemonSet
  template: src=kube-state-metrics.yml.j2 dest="/home/ubuntu/kube-state-metrics.yml"
  when: inventory_hostname in groups['k8s_master']

- name: Check if kube-state-metrics is active
  shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get deployments kube-state-metrics -n kube-system
  changed_when: False
  register: kubectl_state_metrics
  ignore_errors: true
  delegate_to: "{{ groups['k8s_master'][0] }}"
  delegate_facts: True
  # when: "inventory_hostname in groups['k8s_master']"
  tags:
    - skip_ansible_lint # Suppressing the linter

- name: "Show kubectl_state_metrics"
  debug: var=kubectl_state_metrics
  when: debug

- name: Init kube-state-metrics on the first master.
  shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf 
         apply -f /home/ubuntu/kube-state-metrics.yml
  when: "not kubectl_state_metrics.rc == 0
    and inventory_hostname in groups['k8s_master']"
  tags:
    - skip_ansible_lint # Suppressing the linter


- name: start webhook
  set_fact:
    # cluster_api_address: "{{ hostvars[groups['k8s_master'][0]].ansible_default_ipv4.address }}"
    cluster_api_address: "{{ hostvars[groups['k8s_master'][0]]['secondary_ip'] if ('secondary_ip' in hostvars[groups['k8s_master'][0]]) else hostvars[groups['k8s_master'][0]]['ansible_default_ipv4']['address'] }}"
  # when: "inventory_hostname in groups['k8s_master']"

- name: Ensure the /etc/kubernetes/webhook directory exists
  file:
    path: /etc/kubernetes/webhook
    state: directory
    mode: 0755
  when: "inventory_hostname in groups['k8s_master']"

- name: Configure authentication webhook 
  template: src=webhook.kubeconfig.j2 dest="/etc/kubernetes/webhook/webhook.kubeconfig"
  when: inventory_hostname in groups['k8s_master']
  notify:
    - restart kubelet

- name: Add authentication-token-webhook-config-file
  lineinfile:
    regexp: '^    - --authentication-token-webhook-config-file'
    insertafter: '^    - --authorization-mode='
    line: '    - --authentication-token-webhook-config-file=/etc/kubernetes/webhook.kubeconfig'
    path: /etc/kubernetes/manifests/kube-apiserver.yaml
  when: inventory_hostname in groups['k8s_master']
  notify:
    - restart kubelet

- name: Add authorization-webhook-config-file
  lineinfile:
    regexp: '^    - --authorization-webhook-config-file'
    insertafter: '^    - --authorization-mode='
    line: '    - --authorization-webhook-config-file=/etc/kubernetes/webhook.kubeconfig'
    path: /etc/kubernetes/manifests/kube-apiserver.yaml
  when: inventory_hostname in groups['k8s_master']
  notify:
    - restart kubelet

- name: Add authorization-mode
  lineinfile:
    regexp: '^    - --authorization-mode='
    line: '    - --authorization-mode=Node,Webhook,RBAC'
    path: /etc/kubernetes/manifests/kube-apiserver.yaml
  when: inventory_hostname in groups['k8s_master']
  notify:
    - restart kubelet

- name: add webhook mount point
  blockinfile:
    dest: "/etc/kubernetes/manifests/kube-apiserver.yaml"
    insertafter: "    volumeMounts:"
    block: |
      {{ lookup('file', 'roles/k8s/files/webhookmounts.yaml') }}
    marker: "# {mark} ANSIBLE MANAGED BLOCK for mount point"
  when: inventory_hostname in groups['k8s_master']
  notify:
    - restart kubelet

- name: add webhook volume
  blockinfile:
    dest: "/etc/kubernetes/manifests/kube-apiserver.yaml"
    insertafter: "  volumes:"
    block: |
      {{ lookup('file', 'roles/k8s/files/webhookvolumes.yaml') }}
    marker: "# {mark} ANSIBLE MANAGED BLOCK for volume"
  when: inventory_hostname in groups['k8s_master']
  notify:
    - restart kubelet

- name: Configure authentication webhook DaemonSet
  template: src=webhook.yml.j2 dest="/home/ubuntu/webhook.yml"
  when: inventory_hostname in groups['k8s_master']

- name: Configure authorisation webhook policy
  template: src=policy.json.j2 dest="/etc/kubernetes/webhook/policy.json"
  when: inventory_hostname in groups['k8s_master']
  notify:
    - restart kubelet

- meta: flush_handlers

- name: Check if Webhook is active
  shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get daemonset k8s-keystone-auth -n kube-system
  changed_when: False
  register: kubectl_webhook
  ignore_errors: true
  delegate_to: "{{ groups['k8s_master'][0] }}"
  delegate_facts: True
  # when: "inventory_hostname in groups['k8s_master']"
  tags:
    - skip_ansible_lint # Suppressing the linter

- name: "Show kubectl_webhook"
  debug: var=kubectl_webhook
  when: debug

- name: Init Webhook on the first master.
  shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf 
         apply -f /home/ubuntu/webhook.yml
  when: "not kubectl_webhook.rc == 0
    and inventory_hostname in groups['k8s_master']"
  tags:
    - skip_ansible_lint # Suppressing the linter

# - name: Add cloud-provider=external
#   lineinfile:
#     regexp: '^    - --cloud-provider='
#     insertafter: '^    - --authorization-webhook-config-file='
#     line: '    - --cloud-provider=external'
#     path: /etc/kubernetes/manifests/kube-apiserver.yaml
#   when: inventory_hostname in groups['k8s_master']
#   notify:
#     - restart kubelet

# - name: Add cloud-provider=openstack
#   lineinfile:
#     regexp: '^    - --cloud-provider='
#     insertafter: '^    - --authorization-webhook-config-file='
#     line: '    - --cloud-provider=openstack'
#     path: /etc/kubernetes/manifests/kube-apiserver.yaml
#   when: inventory_hostname in groups['k8s_master']
#   notify:
#     - restart kubelet

# - name: Add cloud-provider config
#   lineinfile:
#     regexp: '^    - --cloud-config='
#     insertafter: '^    - --cloud-provider='
#     line: '    - --cloud-config=/etc/kubernetes/cloud.conf'
#     path: /etc/kubernetes/manifests/kube-apiserver.yaml
#   when: inventory_hostname in groups['k8s_master']
#   notify:
#     - restart kubelet

# - name: add cloud-config mount point
#   blockinfile:
#     dest: "/etc/kubernetes/manifests/kube-apiserver.yaml"
#     insertafter: "    volumeMounts:"
#     block: |
#       {{ lookup('file', 'roles/k8s/files/cloudconfigmounts.yaml') }}
#     marker: "# {mark} ANSIBLE MANAGED BLOCK for cloud-config mount point"
#   when: inventory_hostname in groups['k8s_master']
#   notify:
#     - restart kubelet

# - name: add cloud-config volume
#   blockinfile:
#     dest: "/etc/kubernetes/manifests/kube-apiserver.yaml"
#     insertafter: "  volumes:"
#     block: |
#       {{ lookup('file', 'roles/k8s/files/cloudconfigvolumes.yaml') }}
#     marker: "# {mark} ANSIBLE MANAGED BLOCK for cloud-config volume"
#   when: inventory_hostname in groups['k8s_master']
#   notify:
#     - restart kubelet

# - name: Add cloud-provider=external kcm
#   lineinfile:
#     regexp: '^    - --cloud-provider='
#     insertafter: '^    - kube-controller-manager'
#     line: '    - --cloud-provider=external'
#     path: /etc/kubernetes/manifests/kube-controller-manager.yaml
#   when: inventory_hostname in groups['k8s_master']
#   notify:
#     - restart kubelet

# - name: Add cloud-provider=openstack kcm
#   lineinfile:
#     regexp: '^    - --cloud-provider='
#     insertafter: '^    - kube-controller-manager'
#     line: '    - --cloud-provider=openstack'
#     path: /etc/kubernetes/manifests/kube-controller-manager.yaml
#   when: inventory_hostname in groups['k8s_master']
#   notify:
#     - restart kubelet

# - name: Add cloud-provider config kcm
#   lineinfile:
#     regexp: '^    - --cloud-config='
#     insertafter: '^    - --cloud-provider='
#     line: '    - --cloud-config=/etc/kubernetes/cloud.conf'
#     path: /etc/kubernetes/manifests/kube-controller-manager.yaml
#   when: inventory_hostname in groups['k8s_master']
#   notify:
#     - restart kubelet

# - name: add cloud-config mount point kcm
#   blockinfile:
#     dest: "/etc/kubernetes/manifests/kube-controller-manager.yaml"
#     insertafter: "    volumeMounts:"
#     block: |
#       {{ lookup('file', 'roles/k8s/files/cloudconfigmounts.yaml') }}
#     marker: "# {mark} ANSIBLE MANAGED BLOCK for cloud-config mount point"
#   when: inventory_hostname in groups['k8s_master']
#   notify:
#     - restart kubelet

# - name: add cloud-config volume kcm
#   blockinfile:
#     dest: "/etc/kubernetes/manifests/kube-controller-manager.yaml"
#     insertafter: "  volumes:"
#     block: |
#       {{ lookup('file', 'roles/k8s/files/cloudconfigvolumes.yaml') }}
#     marker: "# {mark} ANSIBLE MANAGED BLOCK for cloud-config volume"
#   when: inventory_hostname in groups['k8s_master']
#   notify:
#     - restart kubelet



- name: cloud.conf
  template: src=cloud.conf.j2 dest="/etc/kubernetes/cloud.conf"
  tags:
    - k8s_cloud_provider
  notify:
    - restart kubelet



# CCM Documentation
# https://kubernetes.io/docs/admin/extensible-admission-controllers/#enable-initializers-alpha-feature
# https://github.com/docker/for-mac/issues/2771
# https://kubernetes.io/docs/admin/extensible-admission-controllers/#initializers
# https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/

# # deprecation and conflict warnings:
# # admission-control and enable-admission-plugins/disable-admission-plugins flags are mutually exclusive
# # - name: disable PersistentVolumeLabel
# #   blockinfile:
# #     dest: "/etc/kubernetes/manifests/kube-apiserver.yaml"
# #     insertafter: "    - --admission-control="
# #     block: |
# #       {{ lookup('file', 'roles/k8s/files/disablepersistentvolumelabels.yaml') }}
# #     marker: "# {mark} ANSIBLE MANAGED BLOCK for disablepersistentvolumelabels"
# #   when: inventory_hostname in groups['k8s_master']
# #   notify:
# #     - restart kubelet

# - name: Remove admission-control
#   lineinfile:
#     regexp: '^    - --admission-control='
#     state: absent
#     path: /etc/kubernetes/manifests/kube-apiserver.yaml
#   when: inventory_hostname in groups['k8s_master']
#   notify:
#     - restart kubelet

# - name: Add Initializers
#   lineinfile:
#     regexp: '^    - --enable-admission-plugins='
#     insertafter: '^    - --authorization-webhook-config-file='
#     line: '    - --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,Initializers'
#     path: /etc/kubernetes/manifests/kube-apiserver.yaml
#   when: inventory_hostname in groups['k8s_master']
#   notify:
#     - restart kubelet

# - name: disable PersistentVolumeLabel
#   lineinfile:
#     regexp: '^    - --disable-admission-plugins='
#     insertafter: '^    - --enable-admission-plugins='
#     line: '    - --disable-admission-plugins=PersistentVolumeLabel'
#     path: /etc/kubernetes/manifests/kube-apiserver.yaml
#   when: inventory_hostname in groups['k8s_master']
#   notify:
#     - restart kubelet

# - name: Add runtime-config
#   lineinfile:
#     regexp: '^    - --runtime-config='
#     insertafter: '^    - --disable-admission-plugins='
#     line: '    - --runtime-config=admissionregistration.k8s.io/v1alpha1'
#     path: /etc/kubernetes/manifests/kube-apiserver.yaml
#   when: inventory_hostname in groups['k8s_master']
#   notify:
#     - restart kubelet

# - meta: flush_handlers

# - name: persistent-volume-label-initializer-config.yaml
#   template: src=persistent-volume-label-initializer-config.yaml.j2 dest="/home/ubuntu/persistent-volume-label-initializer-config.yaml"
#   when: inventory_hostname in groups['k8s_master']

# - name: Check if initializer is active
#   shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get deployment standalone-cinder-provisioner
#   changed_when: False
#   register: kubectl_initializer
#   ignore_errors: true
#   delegate_to: "{{ groups['k8s_master'][0] }}"
#   delegate_facts: True
#   # when: "inventory_hostname in groups['k8s_master']"
#   tags:
#     - skip_ansible_lint # Suppressing the linter

# - name: "Show kubectl_initializer"
#   debug: var=kubectl_initializer
#   when: debug

# - name: Init initializer on the first master.
#   shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf 
#          apply -f /home/ubuntu/persistent-volume-label-initializer-config.yaml
#   # when: "not kubectl_initializer.rc == 0
#   when: "inventory_hostname in groups['k8s_master']"
#   tags:
#     - skip_ansible_lint # Suppressing the linter

- name: 10-kubeadm.conf
  template: src=10-kubeadm.conf.j2 dest="/etc/systemd/system/kubelet.service.d/10-kubeadm.conf"
  notify:
    - Reload systemd
    - restart kubelet
  tags:
    - k8s_cloud_provider

- meta: flush_handlers

# Role issues
# https://github.com/fabric8io/fabric8/issues/6840


# - name: Configure cinder RBAC
#   template: src=cinder-role.yml.j2 dest="/home/ubuntu/cinder-role.yml"
#   when: inventory_hostname in groups['k8s_master']

# - name: Check if Cinder clusterrolebinding is active
#   shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get clusterrolebindings  cinder-rbac
#   changed_when: False
#   register: kubectl_cinder_role
#   ignore_errors: true
#   delegate_to: "{{ groups['k8s_master'][0] }}"
#   delegate_facts: True
#   # when: "inventory_hostname in groups['k8s_master']"
#   tags:
#     - skip_ansible_lint # Suppressing the linter

# - name: "Show kubectl_cinder_role"
#   debug: var=kubectl_cinder_role
#   when: debug

# - name: Init Cinder clusterrolebinding on the first master.
#   shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf 
#          apply -f /home/ubuntu/cinder-role.yml
#   when: "not kubectl_cinder_role.rc == 0
#     and inventory_hostname in groups['k8s_master']"
#   tags:
#     - skip_ansible_lint # Suppressing the linter


- name: Configure cinder volume manager
  template: src=cinder.yml.j2 dest="/home/ubuntu/cinder.yml"
  when: inventory_hostname in groups['k8s_master']

- name: Check if Provisioner is active
  shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get daemonset standalone-cinder-provisioner -n kube-system
  changed_when: False
  register: kubectl_provisioner
  ignore_errors: true
  delegate_to: "{{ groups['k8s_master'][0] }}"
  delegate_facts: True
  # when: "inventory_hostname in groups['k8s_master']"
  tags:
    - skip_ansible_lint # Suppressing the linter

- name: "Show kubectl_provisioner"
  debug: var=kubectl_provisioner
  when: debug

- name: Init Provisioner on the first master.
  shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf 
         apply -f /home/ubuntu/cinder.yml
  when: "not kubectl_provisioner.rc == 0
    and inventory_hostname in groups['k8s_master']"
  tags:
    - skip_ansible_lint # Suppressing the linter


- name: Create volume Class and Volume for mysql
  template: src=mysql-pv.yml.j2 dest="/home/ubuntu/mysql-pv.yml"
  when: inventory_hostname in groups['k8s_master']


- name: Create pod for mysql
  template: src=mysql.yml.j2 dest="/home/ubuntu/mysql.yml"
  when: inventory_hostname in groups['k8s_master']


# - name: Configure ccm
#   template: src=ccm.yml.j2 dest="/home/ubuntu/ccm.yml"
#   when: inventory_hostname in groups['k8s_master']

# - name: Check if ccm is active
#   shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get deployment openstack-cloud-controller-manager -n kube-system
#   changed_when: False
#   register: kubectl_ccm
#   ignore_errors: true
#   delegate_to: "{{ groups['k8s_master'][0] }}"
#   delegate_facts: True
#   # when: "inventory_hostname in groups['k8s_master']"
#   tags:
#     - skip_ansible_lint # Suppressing the linter

# - name: "Show kubectl_ccm"
#   debug: var=kubectl_ccm
#   when: debug

# - name: Init ccm on the first master.
#   shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf 
#          apply -f /home/ubuntu/ccm.yml
#   when: "not kubectl_ccm.rc == 0
#     and inventory_hostname in groups['k8s_master']"
#   tags:
#     - skip_ansible_lint # Suppressing the linter

# docker run -ti --net=host -v /etc/kubernetes/pki:/etc/kubernetes/pki -v /etc/ssl/certs:/etc/ssl/certs -v /etc/kubernetes/controller-manager.conf:/etc/kubernetes/controller-manager.conf -v /usr/libexec/kubernetes/kubelet-plugins/volume/exec:/usr/libexec/kubernetes/kubelet-plugins/volume/exec -v /etc/kubernetes/cloud.conf:/etc/kubernetes/cloud.conf --rm docker.io/k8scloudprovider/openstack-cloud-controller-manager:latest /bin/openstack-cloud-controller-manager --v=2 --cloud-config=/etc/kubernetes/cloud.conf --cloud-provider=openstack --use-service-account-credentials=true --address=127.0.0.1 --kubeconfig=/etc/kubernetes/controller-manager.conf





# - name: Configure API Server
#   template: src=kube-apiserver.yaml.j2 dest="/etc/kubernetes/manifests/kube-apiserver.yaml"
#   when: inventory_hostname in groups['k8s_master']
#   tags:
#     - k8s_cloud_provider


# - name: Configure KCM
#   template: src=kube-controller-manager.yaml.j2 dest="/etc/kubernetes/manifests/kube-controller-manager.yaml"
#   when: inventory_hostname in groups['k8s_master']
#   tags:
#     - k8s_cloud_provider
